{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e79af889-f2d3-410b-a936-e4cecff0a64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as mlines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71af257-7514-4089-8845-41aa469aaa38",
   "metadata": {},
   "source": [
    "# TSNE\n",
    "\n",
    "Read in supervised classifications and centered and scaled unlabeled data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1ff7c98-d90a-461f-9192-084e0661dc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_LinearSVC = pd.read_pickle('./data/y_pred_LinearSVC.pkl')\n",
    "y_LogReg = pd.read_pickle('./data/y_pred_LogReg.pkl')\n",
    "y_RidgeReg = pd.read_pickle('./data/y_pred_RidgeReg.pkl')\n",
    "X = pd.read_pickle('./data/unlabeled_behavior.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81554759-c7f8-41c8-af61-ca01e53c1a43",
   "metadata": {},
   "source": [
    "Define a plotting function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "603e4d7b-16a2-4fa7-af4a-d38d9391a0df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot_with_ys(x1, x2, y_class, y_cluster=None)\n",
    "# Generates a 2d plot of the given data. If only y_class is given, data points will be colored by\n",
    "# classification. If y_cluster is also given, data points will be colored by cluster label with shapes\n",
    "# corresponding to classification. Intended for use with embeddings generated by TSNE. Plots with\n",
    "# extended names will go to a scratch folder so we can compare selections of parameters for TSNE.\n",
    "# Once we've chosen parameters these will be used through all future plots so the plots that go\n",
    "# into our report can go without extended names.\n",
    "# Variables:\n",
    "# x1        -  array representing the position on the x-axis of each point in a 2d embedding from TSNE\n",
    "# x2        -  array representing the position on the y-axis of each point in a 2d embedding from TSNE\n",
    "# names     -  an array of string names for the classification algorithm used (index 0) and\n",
    "#              the clustering algorithm used (index 1). If ext_names=True, also includes the \n",
    "#              perplexity (index 2) and the distance metric (index 3). For use in plotting.\n",
    "# y_class   -  an array of classifications from a supervised model\n",
    "# y_cluster -  an array of cluster labelings (default: None)\n",
    "# ext_names -  whether to look for extended names for the plot (default: False)\n",
    "@mpl.rc_context({'image.cmap': 'tab10', 'figure.figsize': [12.0, 8.0]})\n",
    "def plot_with_ys(x1, x2, names, y_class, y_cluster=None, ext_names=False):\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    if y_cluster is not None: #plotting comparison between classification and clustering\n",
    "        \n",
    "        #create a colormap of the correct size, doing this bc just giving 'tab10' to the cmap\n",
    "        #parameter gives colors from each end of the palette instead of sequentially\n",
    "        colors = mpl.colors.ListedColormap(plt.get_cmap('tab10')(np.arange(len(np.unique(y_cluster)))))\n",
    "        \n",
    "        #plot the two predicted classes with different markers, coloring by cluster assignment\n",
    "        x1_normal = [a for a,b in zip(x1, y_class) if b == 0]\n",
    "        x2_normal = [a for a,b in zip(x2, y_class) if b == 0]\n",
    "        scatter1 = ax.scatter(x1_normal, x2_normal, marker='|', cmap=colors,\n",
    "                              c=y_cluster[np.argwhere(y_class == 0)])\n",
    "        \n",
    "        x1_outlier = [a for a,b in zip(x1, y_class) if b == 1]\n",
    "        x2_outlier = [a for a,b in zip(x2, y_class) if b == 1]\n",
    "        scatter2 = ax.scatter(x1_outlier, x2_outlier, marker='_', cmap=colors,\n",
    "                              c=y_cluster[np.argwhere(y_class == 1)])\n",
    "        \n",
    "        #create a legend for differentiating between colors\n",
    "        legend1 = ax.legend(*scatter1.legend_elements(), loc=\"lower left\", title=\"Clusters\")\n",
    "        ax.add_artist(legend1)\n",
    "        \n",
    "        #create a legend from scratch for differentiating between markers\n",
    "        vline = mlines.Line2D([], [], color='black', marker='|', linestyle='None',\n",
    "                              markersize=10, label='0 (normal)')\n",
    "        hline = mlines.Line2D([], [], color='black', marker='_', linestyle='None',\n",
    "                              markersize=10, label='1 (outlier)')\n",
    "        legend2 = ax.legend(handles=[vline, hline], loc=\"lower right\", title=\"Classes\")\n",
    "        \n",
    "        #insert given names to title and filename\n",
    "        if ext_names: #with names for perplexity and distance metric\n",
    "            plt.title('TSNE '+names[1]+' Clusterings & ' +names[0]+' Classifications p'\n",
    "                      +names[2]+' '+names[3])\n",
    "            filename = './figures/scratch/TSNE_'+names[1]+'_'+names[0]+'_p'+names[2]+'_'+names[3]+'.png'\n",
    "        else: #without names for perplexity and distance metric\n",
    "            plt.title('TSNE '+names[1]+' Clusterings & ' +names[0]+' Classifications')\n",
    "            filename = './figures/TSNE_'+names[1]+'_'+names[0]+'.png'\n",
    "\n",
    "        plt.savefig(filename, format='png')\n",
    "    else: #plotting only a classification\n",
    "        colors = mpl.colors.ListedColormap(plt.get_cmap('tab10')(np.arange(2)))\n",
    "        scatter = ax.scatter(x1, x2, marker='.', c=y_class, cmap=colors)\n",
    "        legend = ax.legend(*scatter.legend_elements(), loc=\"lower left\", title=\"Classes\")\n",
    "        \n",
    "        #insert given names to title and filename\n",
    "        if ext_names: #with names for perplexity and distance metric\n",
    "            plt.title('TSNE '+names[0]+' Classifications p'+names[2]+' '+names[3])\n",
    "            filename = './figures/scratch/TSNE_'+names[0]+'_p'+names[2]+'_'+names[3]+'.png'\n",
    "        else: #without names for perplexity and distance metric\n",
    "            plt.title('TSNE '+names[0]+' Classifications')\n",
    "            filename = './figures/TSNE_'+names[0]+'.png'\n",
    "            \n",
    "        plt.savefig(filename, format='png')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d846993-369b-4f4c-b7a5-e3b38f37eeaf",
   "metadata": {},
   "source": [
    "Let's run TSNE with various values of perplexity and distance functions. Why only these two? For one, it takes a little while for each model to run, so we need to cut down on the number of combinations to try. What about the other parameters?\n",
    "\n",
    "- early_exaggeration: according to docs \"the choice of this parameter is not very critical\" so we won't worry about this unless we're having trouble getting reasonable outputs with the default value\n",
    "- learning_rate: the 'auto' option for our dataset works out to about 650, which is reasonable considering the typical range is 10-1000; inital outputs looked good, i.e. results did not look like a uniform ball or a dense cloud with few outliers, so it seems reasonable to continue with this learning rate unless results look particularly bad\n",
    "- method: the exact gradient calculation algorithm crashed my ipykernel so we'll stick with Barnes-Hut approximation\n",
    "- angle: we will leave this at the default 0.5 because initial runs completed in a reasonable time while giving decent results, and Barnes-Hut \"is not very sensitive to changes in this parameter in the range of 0.2 - 0.8\"\n",
    "- max_iter: initial runs seemed to optimize fine at the default of 1000 so we'll leave this be unless runs are failing to complete\n",
    "- init: since initial runs even at high perplexity did not have better outputs with 'random' than 'pca' we will stick with PCA. PCA initialization is also usually more globally stable. However, PCA initialization is not supported for pre-computed distance matrices, which we will need when using the Gower distance, so in this one case we will use random initializations.\n",
    "\n",
    "How did we choose the range of perplexities to try? Initial runs with high perplexity (100, 500) did not result in better separation of the data in accordance to supervised classifications. In fact, high perplexity values resulted in a separation that cut each of the classes in half. I suspect that these embeddings were over-focusing on the differences between data from the two different sources used for the dataset, which is composed of roughly half from each source. We will therefore try embeddings with perplexity from 5 to 65 moving in steps of 10.\n",
    "\n",
    "How did we choose which distance metrics to try? We have 7 centered and scaled continuous variables, and 2 categorical variables with 2 levels each. We will run models with euclidean distance, l2 norm, cosine distance, and correlation distance as available from sklearn and scipy. We will also run models using the Gower distance, which uses Manhattan distance for continuous variables and dice distance for binary variables. The implementation of Gower distance comes from the [gower](https://github.com/wwwjk366/gower) package. The Gower matrix is too large to compute and use on our laptops, once computed on Colab the matrix takes up 4.3 GB in pickle format. Generating embeddings with this matrix goes beyond the Colab RAM allotment, so we'll have to do this on Discovery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2178fb-a1af-439e-b79a-52425f8600ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "perplexities = np.arange(5, 70, 10)\n",
    "distance_metrics = ['euclidean', 'l2', 'cosine', 'correlation']\n",
    "\n",
    "for p in perplexities:\n",
    "    for d in distance_metrics: \n",
    "        #apply TSNE with distance metrics from sklearn and scipy\n",
    "        model = TSNE(n_components=2, perplexity=p, learning_rate='auto', metric=d, init='pca',\n",
    "                     method='barnes_hut', square_distances=True, n_jobs=-1)\n",
    "        X_new = model.fit_transform(X)\n",
    "        plot_with_ys(X_new[:,0], X_new[:,1], ['LinearSVC', None, str(p), d],\n",
    "                     y_LinearSVC, ext_names=True)\n",
    "        plot_with_ys(X_new[:,0], X_new[:,1], ['LogReg', None, str(p), d],\n",
    "                     y_LogReg, ext_names=True)\n",
    "        plot_with_ys(X_new[:,0], X_new[:,1], ['RidgeReg', None, str(p), d],\n",
    "                     y_RidgeReg, ext_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f56ad89-959a-4a80-9a96-52a15276758c",
   "metadata": {},
   "source": [
    "It turns out that Gower distance gave the best embeddings, so final plots and embedding are generated in `gower.ipynb`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
